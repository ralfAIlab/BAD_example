{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports.  {vertical-output: true}\n",
    "\n",
    "!pip install tensorflow==1.15\n",
    "!pip install dm-sonnet==1.36\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title BAD.  {vertical-output: true}\n",
    "\n",
    "def repeat_tensor(tensor, repetion):\n",
    "  exp_tensor = tf.expand_dims(tensor, axis=-1)\n",
    "  tensor_t = tf.tile(exp_tensor, [1] + repetion)\n",
    "  tensor_r = tf.reshape(tensor_t, repetion * tf.shape(tensor))\n",
    "  return tensor_r\n",
    "\n",
    "\n",
    "def get_ops(bad_mode, payoff_values, batch_size, num_hidden=32):\n",
    "  # Input is a single number for agent 0 and agent 1.\n",
    "  input_0 = tf.placeholder(tf.int32, shape=batch_size)\n",
    "  input_1 = tf.placeholder(tf.int32, shape=batch_size)\n",
    "\n",
    "  # Payoff matrix.\n",
    "  num_cards = payoff_values.shape[0]     # C.\n",
    "  num_actions = payoff_values.shape[-1]  # A.\n",
    "  payoff_tensor = tf.constant(payoff_values)\n",
    "\n",
    "  # Agent 0.\n",
    "  with tf.variable_scope('agent_0'):\n",
    "    weights_0 = tf.get_variable('weights_0', shape=(num_cards, num_actions))\n",
    "    baseline_0_mlp = snt.nets.MLP([num_hidden, 1]) \n",
    "\n",
    "  # Agent 1.\n",
    "  with tf.variable_scope('agent_1'):\n",
    "    p1 = snt.nets.MLP([num_hidden, num_actions])\n",
    "    baseline_1_mlp = snt.nets.MLP([num_hidden, 1])\n",
    "\n",
    "  # These are the 'counterfactual inputs', i.e., all possible cards.\n",
    "  all_inputs = tf.placeholder(tf.int32, shape=(1, num_cards))\n",
    "\n",
    "  # We repeat them for each batch.\n",
    "  repeated_in = tf.reshape(repeat_tensor(all_inputs, [batch_size, 1]), [-1])\n",
    "\n",
    "  # Next we calculate the counterfactual action and log_p for each batch \n",
    "  # and hand.\n",
    "  log_p0 = tf.nn.log_softmax(\n",
    "      tf.matmul(tf.one_hot(repeated_in, num_cards), weights_0))\n",
    "  cf_action = tf.to_int32(\n",
    "      tf.squeeze(tf.multinomial(log_p0, num_samples=1)))  # [BC].\n",
    "  \n",
    "  # Produce log-prob of action selected.\n",
    "  log_cf = tf.reduce_sum(\n",
    "      log_p0 * tf.one_hot(cf_action, num_actions), axis=-1)  # [BC].\n",
    "\n",
    "  # Some reshaping.\n",
    "  repeated_in = tf.reshape(repeated_in, [batch_size, -1])  # [B,C].\n",
    "  cf_action = tf.reshape(cf_action, [batch_size, -1])  # [B,C].\n",
    "  log_cf = tf.reshape(log_cf, [batch_size, -1])  # [B,C].\n",
    "\n",
    "  # Now we need to know the action the agent actually took. \n",
    "  # This is done by indexing the cf_action with the private observation.\n",
    "  u0 = tf.reduce_sum(\n",
    "      cf_action * tf.one_hot(input_0, num_cards, dtype=tf.int32), axis=-1)\n",
    "\n",
    "  # Do the same for the log-prob.\n",
    "  log_p0 = tf.reduce_sum(log_cf * tf.one_hot(input_0, num_cards), axis=-1)\n",
    "\n",
    "  # Joint-action includes all the counterfactual probs - it's simply the sum.\n",
    "  joint_log_p0 = tf.reduce_sum(log_cf, axis=-1)\n",
    "\n",
    "  # Repeating the action chosen so that we can check all matches.\n",
    "  repeated_actions = repeat_tensor(\n",
    "      tf.reshape(u0, [batch_size, 1]), [1, num_cards])\n",
    "\n",
    "  # A hand is possible iff the action in that hand matches the action chosen.\n",
    "  weights = tf.to_int32(tf.equal(cf_action, repeated_actions))\n",
    "\n",
    "  # Normalize beliefs to sum to 1.\n",
    "  beliefs = tf.to_float(\n",
    "      tf.divide(weights, tf.reduce_sum(weights, axis=-1, keepdims=True))) \n",
    "\n",
    "  # Stop gradient mostly as a precaution.\n",
    "  beliefs = tf.stop_gradient(beliefs)\n",
    "\n",
    "  # Agent 1 receives beliefs + private ops for agent 1, unless it's \n",
    "  # the pure policy gradient version.\n",
    "  if bad_mode == 2:\n",
    "    joint_in1 = tf.concat([\n",
    "        tf.one_hot(u0, num_actions, dtype=tf.float32), \n",
    "        tf.one_hot(input_1, num_cards, dtype=tf.float32),\n",
    "    ], axis=1)\n",
    "  else:\n",
    "    joint_in1 = tf.concat([\n",
    "        tf.one_hot(u0, num_actions, dtype=tf.float32),\n",
    "        beliefs,\n",
    "        tf.one_hot(input_1, num_cards, dtype=tf.float32),\n",
    "    ], axis=1)\n",
    "  joint_in1 = tf.reshape(joint_in1, [batch_size, -1])\n",
    "\n",
    "  # We use a centralised baseline that contains both cards as input.\n",
    "  baseline_0_input = tf.concat(\n",
    "      [tf.one_hot(input_0, num_cards), tf.one_hot(input_1, num_cards)], axis=1) \n",
    "  baseline_1_input = tf.concat(\n",
    "      [tf.one_hot(input_0, num_cards), joint_in1], axis=1)\n",
    "\n",
    "  # Calculate baselines.\n",
    "  baseline_0 = tf.squeeze( baseline_0_mlp(baseline_0_input) )\n",
    "  baseline_1 = tf.squeeze( baseline_1_mlp(baseline_1_input))\n",
    "  # Giving the beliefs a fixed shape so that sonnet doesn't complain \n",
    "  # (probably there's a better way).\n",
    "  beliefs = tf.reshape(beliefs, [batch_size, num_cards])\n",
    "\n",
    "  # Evaluate policy for agent 1.\n",
    "  log_p1 = tf.cast(tf.nn.log_softmax(p1(joint_in1)), tf.float32)\n",
    "\n",
    "  # Sample agent 1 and get log-prob of action selected.\n",
    "  u1 = tf.to_int32(tf.squeeze(tf.multinomial(log_p1, num_samples=1)))\n",
    "  log_p1 = tf.reduce_sum(log_p1 * tf.one_hot(u1, num_actions), axis=-1)\n",
    "\n",
    "  # Getting the rewards is just indexing into the payout matrix for all \n",
    "  # elements in the batch.\n",
    "  rewards = tf.stack([\n",
    "      payoff_tensor[input_0[i], input_1[i], u0[i], u1[i]] \n",
    "      for i in range(batch_size)\n",
    "  ], axis=0)\n",
    "\n",
    "  # Log-prob used for learning.\n",
    "  if bad_mode == 1:\n",
    "    log_p0_train = joint_log_p0\n",
    "  else:\n",
    "    log_p0_train = log_p0\n",
    "  log_p1_train = log_p1\n",
    "\n",
    "  # Policy-gradient loss.\n",
    "  pg_final = tf.reduce_mean(\n",
    "      (rewards - tf.stop_gradient(baseline_0)) * log_p0_train)\n",
    "  pg_final += tf.reduce_mean(\n",
    "      (rewards - tf.stop_gradient(baseline_1)) * log_p1_train)\n",
    "\n",
    "  # Baseline loss.\n",
    "  total_baseline_loss = tf.reduce_mean(tf.square(rewards - baseline_0)) \n",
    "  total_baseline_loss += tf.reduce_mean(tf.square(rewards - baseline_1)) \n",
    "\n",
    "  # Train policy.\n",
    "  opt_policy = tf.train.AdamOptimizer()\n",
    "  train_policy = opt_policy.minimize(-pg_final)\n",
    "  \n",
    "  # Train baseline.\n",
    "  opt_baseline = tf.train.AdamOptimizer()\n",
    "  train_baseline = opt_baseline.minimize(total_baseline_loss)\n",
    "  \n",
    "  # Pack up the placeholders.\n",
    "  phs = {\n",
    "      'input_0': input_0,\n",
    "      'input_1': input_1,\n",
    "      'all_inputs': all_inputs,\n",
    "  }\n",
    "\n",
    "  # Pack up the train ops.\n",
    "  train_ops = {\n",
    "      'policy': train_policy,\n",
    "      'baseline': train_baseline,     \n",
    "  }\n",
    "  \n",
    "  return rewards, train_ops, phs\n",
    "\n",
    "\n",
    "def train(bad_mode,\n",
    "          batch_size=32,\n",
    "          num_runs=1,\n",
    "          num_episodes=5000,\n",
    "          num_readings=100,\n",
    "          seed=42,\n",
    "          debug=False):\n",
    "  # Payoff values, [C,C,A,A].\n",
    "  payoff_values = np.asarray([\n",
    "      [\n",
    "          [[10, 0, 0], [4, 8, 4], [10, 0, 0]],\n",
    "          [[0, 0, 10], [4, 8, 4], [0, 0, 10]],\n",
    "      ],\n",
    "      [\n",
    "          [[0, 0, 10], [4, 8, 4], [0, 0, 0]],\n",
    "          [[10, 0, 0], [4, 8, 4], [10, 0, 0]],\n",
    "      ],\n",
    "  ], dtype=np.float32)\n",
    "  num_cards = payoff_values.shape[0]  # C.\n",
    "\n",
    "  # All cards.\n",
    "  all_cards = np.zeros((1, num_cards))\n",
    "  for i in range(num_cards):\n",
    "    all_cards[0, i] = i\n",
    "\n",
    "  # Reset TF graph.\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # Set random number generator seeds for reproducibility.\n",
    "  tf.set_random_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  \n",
    "  # Build graph.\n",
    "  rewards_op, train_ops, phs = get_ops(bad_mode, payoff_values, batch_size)\n",
    "  \n",
    "  # Initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "  \n",
    "  # Run.\n",
    "  rewards = np.zeros((num_runs, num_readings + 1))\n",
    "  interval = num_episodes // num_readings\n",
    "  with tf.Session() as sess:      \n",
    "    for run_id in range(num_runs):\n",
    "      if run_id % max(num_runs // 10, 1) == 0:\n",
    "        print('Run {}/{} ...'.format(run_id + 1, num_runs))\n",
    "      \n",
    "      sess.run(init)\n",
    "      for episode_id in range(num_episodes + 1):\n",
    "        cards_0 = np.random.choice(num_cards, size=batch_size)\n",
    "        cards_1 = np.random.choice(num_cards, size=batch_size)\n",
    "\n",
    "        fetches = [rewards_op, train_ops['baseline'], train_ops['policy']]\n",
    "        feed_dict = {\n",
    "            phs['input_0']: cards_0,\n",
    "            phs['input_1']: cards_1,\n",
    "            phs['all_inputs']: all_cards,\n",
    "        }\n",
    "        reward = sess.run(fetches, feed_dict)[:-2]  # Ignore train ops.\n",
    "        reward = np.mean(reward)  # Average over batch.\n",
    "\n",
    "        # Maybe save.\n",
    "        if episode_id % interval == 0:\n",
    "          rewards[run_id, episode_id // interval] = reward\n",
    "\n",
    "        # Maybe log.\n",
    "        if debug and episode_id % (num_episodes // 5) == 0:\n",
    "          print(episode_id, 'reward:', reward)\n",
    "\n",
    "  return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train.  {vertical-output: true}\n",
    "\n",
    "mode_labels = [\n",
    "    'BAD, no CF gradient',\n",
    "    'BAD, with CF gradient', \n",
    "    'Vanilla PG',\n",
    "]\n",
    "\n",
    "# Set debug = True to get a faster run (roughly 5 mins) and more printouts.\n",
    "debug = False\n",
    "\n",
    "if debug:\n",
    "  num_runs = 3\n",
    "  num_episodes = 5000\n",
    "else:\n",
    "  num_runs = 30\n",
    "  num_episodes = 15000\n",
    "num_readings = 100\n",
    "\n",
    "rewards_by_bad_mode = {}\n",
    "for bad_mode in range(3):\n",
    "  print('Running', mode_labels[bad_mode])\n",
    "  rewards_by_bad_mode[bad_mode] = train(bad_mode,\n",
    "                                        num_runs=num_runs,\n",
    "                                        num_episodes=num_episodes,\n",
    "                                        num_readings=num_readings,\n",
    "                                        debug=debug)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot training curves.  {vertical-output: true}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "save_every = num_episodes // num_readings\n",
    "steps = np.arange(num_readings + 1) * save_every\n",
    "for bad_mode in range(3):\n",
    "  rewards = rewards_by_bad_mode[bad_mode]\n",
    "  mean = rewards.mean(axis=0)\n",
    "  sem = rewards.std(axis=0) / np.sqrt(num_runs)\n",
    "  plt.plot(steps, mean, label=mode_labels[bad_mode])\n",
    "  plt.fill_between(steps, mean - sem, mean + sem, alpha=0.3)\n",
    "plt.ylim(8, 9.7)\n",
    "plt.legend()\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
